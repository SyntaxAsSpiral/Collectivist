# üíÆ Collectivist Configuration
# Copy this file to one of these locations:
# 1. .collection/collectivist.yaml (collection-specific config)
# 2. ~/.collectivist/config.yaml (global user config)
# 3. Or specify custom path with --config option

# =============================================================================
# ü§ñ LLM CONFIGURATION
# =============================================================================

# Choose LLM provider: lmstudio, ollama, openrouter, openai, anthropic, pollinations
llm_provider: lmstudio

# Optional: Specify exact model (uses smart defaults if not set)
# llm_model: llama3.1-8b-instruct

# =============================================================================
# üîë API KEYS (Required for cloud providers)
# =============================================================================

# For OpenRouter
# llm_api_key: sk-or-v1-your-openrouter-key

# For OpenAI
# llm_api_key: sk-your-openai-key

# For Anthropic
# llm_api_key: sk-ant-your-anthropic-key

# =============================================================================
# üåê CUSTOM ENDPOINTS (Optional - overrides default provider URLs)
# =============================================================================

# llm_base_url: https://custom-endpoint.com/v1

# =============================================================================
# üìö CONFIGURATION EXAMPLES BY PROVIDER
# =============================================================================

# -----------------------------------------------------------------------------
# üè† LMSTUDIO (Local, Recommended - No API Key Needed)
# Install LMStudio, load a model, and start the local server
# -----------------------------------------------------------------------------
# llm_provider: lmstudio
# llm_model: llama-3.1-8b-instruct-q4_0  # Whatever you have loaded

# -----------------------------------------------------------------------------
# üê≥ OLLAMA (Local - No API Key Needed)
# Install Ollama, pull models with 'ollama pull <model>'
# -----------------------------------------------------------------------------
# llm_provider: ollama
# llm_model: llama3.1:8b-instruct-q4_0  # Exact model name

# -----------------------------------------------------------------------------
# ‚òÅÔ∏è OPENROUTER (Cloud - API Key Required)
# Access to 100+ models through unified API
# -----------------------------------------------------------------------------
# llm_provider: openrouter
# llm_api_key: sk-or-v1-...
# llm_model: meta-llama/llama-3.1-8b-instruct  # Provider/model format
# llm_model: anthropic/claude-3-haiku
# llm_model: openai/gpt-4o-mini

# -----------------------------------------------------------------------------
# ‚ö° OPENAI (Cloud - API Key Required)
# Direct access to OpenAI models
# -----------------------------------------------------------------------------
# llm_provider: openai
# llm_api_key: sk-...
# llm_model: gpt-4o-mini     # Cost-effective default
# llm_model: gpt-4-turbo     # Most capable
# llm_model: gpt-4o          # Latest GPT-4 Optimized

# -----------------------------------------------------------------------------
# üß† ANTHROPIC (Cloud - API Key Required)
# Claude models through official API
# -----------------------------------------------------------------------------
# llm_provider: anthropic
# llm_api_key: sk-ant-...
# llm_model: claude-3-haiku-20240307     # Fast & cheap
# llm_model: claude-3-sonnet-20240229    # Best quality
# llm_model: claude-3-opus-20240229      # Most capable

# -----------------------------------------------------------------------------
# üé® POLLINATIONS (Specialized - No API Key)
# Image generation focused, experimental
# -----------------------------------------------------------------------------
# llm_provider: pollinations
# llm_model: openai  # or anthropic

# =============================================================================
# üß† SMART DEFAULTS (Used when llm_model is not specified)
# =============================================================================
#
# üè† LMStudio: local-model (whatever is loaded in LMStudio)
# üê≥ Ollama: llama3.1 (modern, capable model)
# ‚òÅÔ∏è OpenRouter: meta-llama/llama-3.1-8b-instruct (good balance)
# ‚ö° OpenAI: gpt-4o-mini (cost-effective default)
# üß† Anthropic: claude-3-haiku-20240307 (fast & reliable)
# üé® Pollination: openai

# =============================================================================
# üß™ TESTING YOUR CONFIGURATION
# =============================================================================
#
# Test your LLM connection:
# python -c "from templates.collection-minimal.llm import LLMClient; print('‚úÖ Connected!' if LLMClient.from_config().test_connection() else '‚ùå Failed to connect')"
#
# List available models (if supported by provider):
# python -c "from templates.collection-minimal.llm import LLMClient; print(LLMClient.from_config().get_available_models())"
#
# Copy this file to .collection/collectivist.yaml and customize for your setup!